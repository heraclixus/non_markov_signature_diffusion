seed: 42

data:
  dataset: cifar10
  root: data
  batch_size: 64  # Reduced batch size for non-Markov (more memory intensive)
  num_workers: 4
  image_size: 32

diffusion:
  T: 1000
  schedule: cosine
  cosine_s: 0.008
  num_suffix_steps: 10  # Number of future timesteps in suffix

model:
  in_channels: 3  # RGB images
  base_channels: 128  # Increased for more complex dataset
  channel_mults: [1, 2, 2, 2]
  num_res_blocks: 2
  time_emb_dim: 512
  context_dim: 512  # Dimension of suffix context embedding

encoder:
  hidden_dim: 512
  num_heads: 8
  num_layers: 4
  pooling: mean  # mean, max, or cls

training:
  epochs: 100  # ~40K steps (50K/64 = 781 steps/epoch, 50*781 â‰ˆ 39K steps)
  lr: 2.0e-4
  weight_decay: 0.0
  log_every: 100
  sample_every_steps: 1000
  save_every_steps: 5000
  out_dir: experiments/nonmarkov_ddim_cifar10
  ema_decay: 0.9999
  loss_type: epsilon  # 'epsilon' (predict noise) or 'dart' (predict x0)
  weighting: none  # For epsilon: 'none' (uniform); for dart: 'snr_sum', 'truncated_snr', etc.
  use_cfg: false  # Disable CFG for epsilon prediction baseline
  context_dropout: 0.0

sampling:
  cfg_scale: 0.0  # No CFG


