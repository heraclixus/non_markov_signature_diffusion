seed: 42

data:
  dataset: mnist
  root: data
  batch_size: 128
  num_workers: 4
  image_size: 28

diffusion:
  T: 1000
  schedule: cosine
  cosine_s: 0.008
  num_suffix_steps: 10  # Number of future timesteps in suffix

model:
  in_channels: 1
  base_channels: 64
  channel_mults: [1, 2, 4]
  num_res_blocks: 2
  time_emb_dim: 256
  context_dim: 256  # Dimension of suffix context embedding
  prediction_type: x0  # 'x0' for DART, 'epsilon' for standard

encoder:
  type: signature  # 'transformer' or 'signature'
  # Signature encoder params
  signature_degree: 3  # Truncation level (2-4 typical)
  pooling: spatial_mean  # 'spatial_mean', 'flatten', 'conv_pool'
  time_augment: true  # Add time as an extra channel
  use_lead_lag: false  # Lead-lag transformation (doubles dimension)
  hidden_dim: 256  # For non-spatial_mean pooling

training:
  epochs: 30
  lr: 2.0e-4
  weight_decay: 0.0
  log_every: 100
  sample_every_steps: 1000
  save_every_steps: 2000
  out_dir: experiments/dart_signature
  ema_decay: 0.999
  loss_type: dart  # 'dart' or 'epsilon'
  weighting: snr_sum  # Weighting scheme: 'none', 'simple', 'snr_sum', 'truncated_snr'
  use_cfg: true  # Enable classifier-free guidance training
  context_dropout: 0.1  # CFG training: probability of dropping context (0.1 = 10%, only if use_cfg=true)

sampling:
  cfg_scale: 1.5  # Default CFG scale for sampling (0=off, 1-2=typical)

